# -*- coding: utf-8 -*-
"""Drug_Discovery.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18qK_dgI1Ydttg4qF0tPyhvbYlf66v_4O
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore")

import matplotlib.pyplot as plt
import seaborn as sns

import ipywidgets
from ipywidgets import interact

plt.rcParams['figure.figsize'] = (14, 8)
plt.style.use('fivethirtyeight')

train = pd.read_csv("drugsComTrain_raw.csv")

#test = pd.read_csv("drugsComTest_raw.csv")

#test.head()

final=train.copy()

train.head(3)

train.shape

final = final[final["condition"].str.contains("</span> users found this comment helpful.") == False]

train = train[train["condition"].str.contains("</span> users found this comment helpful.") == False]

final.shape

final.info()

train.drugName.unique()

train.drugName.nunique()

train.condition.unique()



train.condition.nunique()

train.describe()

train[train['usefulCount'] == 0]

train[train['usefulCount'] == 0].count()[0]

train[(train['usefulCount'] == 0) & train['rating'] >= 8].count()[0]

train[train['usefulCount'] > 1000]

train[train['usefulCount'] > 1000].count()[0]

train[train['usefulCount'] > 1000]['rating'].mean()

train[train['usefulCount'] > 1000][['drugName','condition']].reset_index(drop = True)

train[['drugName','condition','review']].describe(include = 'object')

train.condition.isnull().sum()

train = train.dropna()

final=final.dropna()

final.isnull().sum()

plt.scatter(train.rating, train.usefulCount)
plt.xlabel('Rating')
plt.ylabel('Useful Count')
plt.title('Rating vs Useful Count')

train.rating.value_counts()

train.rating.value_counts().plot(kind = "bar")
plt.xlabel("Ratings")
plt.ylabel("Count")

train['len']  = train['review'].apply(len)

train[['rating','len']].groupby(['rating']).agg(['min','mean','max'])

train['review'][train['len'] == train['len'].max()].iloc[0]

train['len'].max()

import string

string.punctuation

# First lets remove Punctuations from the Reviews

def punctuation_removal(messy_str):
    clean_list = [char for char in messy_str if char not in string.punctuation]
    clean_str = ''.join(clean_list)
    return clean_str

train['review'] = train['review'].apply(punctuation_removal)

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

import nltk
nltk.download('stopwords')

import nltk
nltk.download('punkt')

# Now lets Remove the Stopwords also

stop = stopwords.words('english')
stop.append("i'm")

stop_words = []

for item in stop: 
    new_item = punctuation_removal(item)
    stop_words.append(new_item) 

def stopwords_removal(messy_str):
    messy_str = word_tokenize(messy_str)
    return [word.lower() for word in messy_str 
            if word.lower() not in stop_words ]

stop_words

train['review'] = train['review'].apply(stopwords_removal)

# lets remove the Numbers also

import re
def drop_numbers(list_text):
    list_text_new = []
    for i in list_text:
        if not re.search('\d', i):
            list_text_new.append(i)
    return ' '.join(list_text_new)

train['review'] = train['review'].apply(drop_numbers)

# for using Sentiment Analyzer we will have to dowload the Vader Lexicon from NLTK
#(Valence Aware Dictionary and Sentiment Reasoner)

import nltk
nltk.download('vader_lexicon')

# lets calculate the Sentiment from Reviews

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()

train_sentiments = []

for i in train['review']:
    train_sentiments.append(sid.polarity_scores(i).get('compound'))
    
train_sentiments = np.asarray(train_sentiments)
train['sentiment'] = pd.Series(data=train_sentiments)

train[['rating','sentiment']].groupby(['rating']).agg(['min','mean','max'])

#drop the sentiment column and remove the unique Id, date, review, len, and sentiment column also
train = train.drop(['date','uniqueID','sentiment','review','len'], axis = 1)

# lets check the name of columns now
train.columns

min_rating = train['rating'].min()
max_rating = train['rating'].max()
min_rating

max_rating

rating = 1
rating -= min_rating
#rating = rating/(max_rating -1)
#rating *= 5
#rating = int(round(rating,0))

def scale_rating(rating):
    rating -= min_rating
    rating = rating/(max_rating -1)
    rating *= 5
    rating = int(round(rating,0))
    
    if(int(rating) == 0 or int(rating)==1 or int(rating)==2):
        return 0
    else:
        return 1

train['eff_score'] = train['rating'].apply(scale_rating)



train.sample(3)

# lets also calculate Usefulness Score

train['usefulness'] = train['rating']*train['usefulCount']*train['eff_score']

# lets check the Top 10 Most Useful Drugs with their Respective Conditions
train[['drugName','condition','usefulness']][train['usefulness'] > train['usefulness'].mean()].sort_values(by = 'usefulness', ascending = False).head(10).reset_index(drop = True)

# lets calculate the Number of Useless and Useful Drugs for Each Condition

@interact
def check(condition = list(train['condition'].value_counts().index)):
    return train[train['condition'] == condition]['eff_score'].value_counts()

# lets check the Most Common Conditions

train['condition'].nunique()

train['condition'].value_counts().sample(20)

# lets check Drugs, which were useful to Highest Number of Poeple
train[['condition','drugName','usefulCount']][train['usefulCount'] > train['usefulCount'].mean()].sort_values(by = 'usefulCount', ascending = False).head(10).reset_index(drop = True)

train = train.drop_duplicates()

@interact
def high_low_rate(condition = sorted((list(train['condition'].value_counts().index)))):
    print("\n Top 5 Drugs\n Consult Physician before taking any drugs")
    print(train[train['condition'] == condition][['drugName','usefulness','rating']].sort_values(by = 'usefulness',
                        ascending = False).head().reset_index(drop = True))

